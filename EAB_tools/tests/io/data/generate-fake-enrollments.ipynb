{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This code was mostly created by ChatGPT\n",
    "from __future__ import annotations\n",
    "\n",
    "import random\n",
    "from typing import Any\n",
    "\n",
    "from faker import Faker\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "locales_dict: dict[str, float] = {\n",
    "    \"en_US\": 90,\n",
    "    \"es_MX\": 5,\n",
    "    \"en_CA\": 2,\n",
    "    \"en_GB\": 1,\n",
    "    \"fr_FR\": 1,\n",
    "    \"de_DE\": 1,\n",
    "}\n",
    "fake = Faker(locales_dict)\n",
    "np.random.seed(42)  # Also sets the random seed for `pandas`\n",
    "Faker.seed(42)\n",
    "random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_records = 85_253"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sections_per_course_distribution: dict[float, float] = {\n",
    "    1: 0.5288,\n",
    "    2: 0.2028,\n",
    "    3: 0.0946,\n",
    "    4: 0.0601,\n",
    "    5: 0.0274,\n",
    "    6: 0.0286,\n",
    "    7: 0.0211,\n",
    "    8: 0.0033,\n",
    "    9: 0.0030,\n",
    "    10: 0.0051,\n",
    "    11: 0.0079,\n",
    "    12: 0.0031,\n",
    "    13: 0.0032,\n",
    "    15: 0.0029,\n",
    "    18: 0.0014,\n",
    "    19: 0.0016,\n",
    "    20: 0.0026,\n",
    "    24: 0.0012,\n",
    "    35: 0.0013,\n",
    "}\n",
    "\n",
    "instructors_per_course_distribution: dict[float, float] = {\n",
    "    0: 0.0609,\n",
    "    1: 0.9224,\n",
    "    2: 0.0168,\n",
    "}\n",
    "\n",
    "sections_per_instructor_distribution: dict[float, float] = {\n",
    "    1: 0.3148,\n",
    "    2: 0.3148,\n",
    "    3: 0.1872,\n",
    "    4: 0.0922,\n",
    "    5: 0.0425,\n",
    "    6: 0.0192,\n",
    "    7: 0.0149,\n",
    "    8: 0.0055,\n",
    "    9: 0.0036,\n",
    "    10: 0.0035,\n",
    "    15: 0.0018,\n",
    "}\n",
    "\n",
    "students_per_section_distribution: dict[float, float] = {\n",
    "    1: 0.0579,\n",
    "    2: 0.0373,\n",
    "    3: 0.0290,\n",
    "    4: 0.0413,\n",
    "    5: 0.0284,\n",
    "    6: 0.0472,\n",
    "    7: 0.0332,\n",
    "    8: 0.0263,\n",
    "    9: 0.0402,\n",
    "    10: 0.0409,\n",
    "    11: 0.0329,\n",
    "    12: 0.0358,\n",
    "    13: 0.0276,\n",
    "    14: 0.0274,\n",
    "    15: 0.0225,\n",
    "    16: 0.0326,\n",
    "    17: 0.0298,\n",
    "    18: 0.0215,\n",
    "    19: 0.0219,\n",
    "    20: 0.0158,\n",
    "    21: 0.0175,\n",
    "    22: 0.0194,\n",
    "    23: 0.0137,\n",
    "    24: 0.0176,\n",
    "    25: 0.0211,\n",
    "    26: 0.0209,\n",
    "    27: 0.0094,\n",
    "    28: 0.0167,\n",
    "    29: 0.0246,\n",
    "    30: 0.0110,\n",
    "    31: 0.0151,\n",
    "    32: 0.0112,\n",
    "    33: 0.0122,\n",
    "    34: 0.0130,\n",
    "    35: 0.0148,\n",
    "    36: 0.0157,\n",
    "    37: 0.0073,\n",
    "    38: 0.0159,\n",
    "    39: 0.0080,\n",
    "    40: 0.0091,\n",
    "    41: 0.0053,\n",
    "    42: 0.0057,\n",
    "    43: 0.0065,\n",
    "    44: 0.0040,\n",
    "    45: 0.0016,\n",
    "    46: 0.0033,\n",
    "    47: 0.0046,\n",
    "    48: 0.0027,\n",
    "    49: 0.0015,\n",
    "    50: 0.0040,\n",
    "    51: 0.0035,\n",
    "    52: 0.0016,\n",
    "    53: 0.0012,\n",
    "    54: 0.0019,\n",
    "    55: 0.0010,\n",
    "    56: 0.0020,\n",
    "    57: 0.0020,\n",
    "    59: 0.0013,\n",
    "    60: 0.0006,\n",
    "    63: 0.0005,\n",
    "    69: 0.0012,\n",
    "    111: 0.0005,\n",
    "}\n",
    "\n",
    "classifications_distribution: dict[str, float] = {\n",
    "    \"Foo (Winter 2024)\": 0.6120,\n",
    "    \"Foo (Fall 2023)\": 0.1859,\n",
    "    \"Graduate (Winter 2024)\": 0.0573,\n",
    "    \"None\": 0.0379,\n",
    "    \"Foo (Spring 2024)\": 0.0369,\n",
    "    \"Graduate (Fall 2023)\": 0.0160,\n",
    "    \"Foo (Spring 2023)\": 0.0151,\n",
    "    \"Foo (Summer 2023)\": 0.0130,\n",
    "    \"Foo (Winter 2023)\": 0.0064,\n",
    "    \"Foo (Fall 2022)\": 0.0053,\n",
    "    \"Graduate (Spring 2023)\": 0.0015,\n",
    "    \"Foo (Spring 2022)\": 0.0017,\n",
    "    \"Graduate (Summer 2023)\": 0.0014,\n",
    "    \"Graduate (Spring 2024)\": 0.0013,\n",
    "    \"Graduate (Winter 2023)\": 0.0010,\n",
    "    \"Graduate (Fall 2022)\": 0.0005,\n",
    "    \"Graduate (Summer 2022)\": 0.0005,\n",
    "    \"Foo (Fall 2021)\": 0.0005,\n",
    "    \"Foo (Spring 2020)\": 0.0004,\n",
    "    \"Foo (Fall 2019)\": 0.0003,\n",
    "    \"Foo (Summer 2024)\": 0.0004,\n",
    "    \"Foo (Winter 2022)\": 0.0004,\n",
    "    \"Foo (Summer 2022)\": 0.0003,\n",
    "    \"Graduate (Fall 2018)\": 0.0002,\n",
    "    \"Graduate (Winter 2022)\": 0.0002,\n",
    "    \"Foo (Fall 2020)\": 0.0002,\n",
    "    \"Foo (Winter 2019)\": 0.0002,\n",
    "    \"Foo (Winter 2020)\": 0.0002,\n",
    "    \"Graduate (Winter 2018)\": 0.0002,\n",
    "    \"Graduate (Spring 2021)\": 0.0002,\n",
    "    \"Foo (Winter 2017)\": 0.0002,\n",
    "    \"Graduate (Winter 2021)\": 0.0002,\n",
    "    \"Graduate (Summer 2021)\": 0.0002,\n",
    "    \"Freshman (Winter 2024)\": 0.0001,\n",
    "    \"Foo (Winter 2021)\": 0.0001,\n",
    "    \"Foo (Spring 2016)\": 0.0001,\n",
    "    \"Graduate (Spring 2017)\": 0.0001,\n",
    "    \"Graduate (Summer 2020)\": 0.0001,\n",
    "    \"Graduate (Spring 2020)\": 0.0001,\n",
    "    \"Foo (Spring 2019)\": 0.0001,\n",
    "    \"Graduate (Fall 2020)\": 0.0001,\n",
    "    \"Graduate (Spring 2019)\": 0.0001,\n",
    "    \"Graduate (Spring 2022)\": 0.0001,\n",
    "    \"Foo (Summer 2021)\": 0.0001,\n",
    "    \"Graduate (Fall 2021)\": 0.0001,\n",
    "    \"Foo (Fall 2016)\": 0.0001,\n",
    "    \"Foo (Summer 2019)\": 0.0001,\n",
    "    \"Freshman (Fall 2022)\": 0.0001,\n",
    "    \"Graduate (Summer 2024)\": 0.0001,\n",
    "    \"Foo (Fall 2024)\": 0.0001,\n",
    "    \"Foo (Spring 2021)\": 0.0001,\n",
    "}\n",
    "\n",
    "num_courses_per_student_distribution: dict[float, float] = {\n",
    "    1: 0.1870,\n",
    "    2: 0.2159,\n",
    "    3: 0.2484,\n",
    "    4: 0.1562,\n",
    "    5: 0.0881,\n",
    "    6: 0.0480,\n",
    "    7: 0.0226,\n",
    "    8: 0.0105,\n",
    "    9: 0.0067,\n",
    "    10: 0.0056,\n",
    "    11: 0.0018,\n",
    "    12: 0.0053,\n",
    "    13: 0.0027,\n",
    "    14: 0.0005,\n",
    "    15: 0.0001,\n",
    "    16: 0.0005,\n",
    "    17: 0.0002,\n",
    "}\n",
    "\n",
    "credit_hours_per_student_distribution: dict[float, float] = {\n",
    "    0: 0.14,\n",
    "    1: 0.05,\n",
    "    2: 0.07,\n",
    "    3: 0.26,\n",
    "    4: 0.01,\n",
    "    5: 0.44,\n",
    "    6: 0.02,\n",
    "    12: 0.01,\n",
    "}\n",
    "\n",
    "start_times_distribution: dict[str, float] = {\n",
    "    \"7:00 AM\": 0.01,\n",
    "    \"8:00 AM\": 0.06,\n",
    "    \"9:00 AM\": 0.29,\n",
    "    \"10:00 AM\": 0.06,\n",
    "    \"10:30 AM\": 0.03,\n",
    "    \"11:00 AM\": 0.06,\n",
    "    \"12:00 PM\": 0.01,\n",
    "    \"1:00 PM\": 0.29,\n",
    "    \"2:00 PM\": 0.03,\n",
    "    \"3:00 PM\": 0.05,\n",
    "    \"4:00 PM\": 0.05,\n",
    "    \"4:30 PM\": 0.05,\n",
    "    \"5:00 PM\": 0.02,\n",
    "    \"6:00 PM\": 0.01,\n",
    "    \"6:30 PM\": 0.01,\n",
    "}\n",
    "\n",
    "course_duration_distribution: dict[pd.Timedelta, float] = {\n",
    "    pd.to_timedelta(time): prob\n",
    "    for time, prob in {\n",
    "        \"30 min\": 0.01,\n",
    "        \"1 hr\": 0.03,\n",
    "        \"75 min\": 0.17,\n",
    "        \"2 hr\": 0.05,\n",
    "        \"150 min\": 0.05,\n",
    "        \"3 hr\": 0.56,\n",
    "        \"4 hr\": 0.05,\n",
    "        \"5 hr\": 0.04,\n",
    "        \"6 hr\": 0.02,\n",
    "        \"7 hr\": 0.01,\n",
    "        \"8 hr\": 0.01,\n",
    "    }.items()\n",
    "}\n",
    "\n",
    "class_days_distribution: dict[str | None, float] = {\n",
    "    None: 0.14,\n",
    "    \"MWF\": 0.14,\n",
    "    \"TR\": 0.14,\n",
    "    \"T\": 0.11,\n",
    "    \"R\": 0.10,\n",
    "    \"MW\": 0.10,\n",
    "    \"W\": 0.08,\n",
    "    \"M\": 0.07,\n",
    "    \"MTWRF\": 0.05,\n",
    "    \"Sa\": 0.03,\n",
    "    \"F\": 0.03,\n",
    "    \"WR\": 0.01,\n",
    "    \"MTWR\": 0.01,\n",
    "}\n",
    "\n",
    "assigned_staff_role_probabilities: dict[str, float] = {\n",
    "    \"Advisor\": 0.90,\n",
    "    \"Career Advisor\": 0.80,\n",
    "    \"Professor\": 0.20,\n",
    "    \"Student Finance\": 0.10,\n",
    "    \"FAFSA coordinator\": 0.10,\n",
    "    \"International Success Advisor\": 0.02,\n",
    "    \"Field Advisor\": 0.01,\n",
    "    \"VA coordinator\": 0.01,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_probs_sum_to_one(d: dict[Any, float]) -> dict[Any, float]:\n",
    "    \"\"\"Make sure the probabilities in dict values add up to one\"\"\"\n",
    "    return {key: value / sum(d.values()) for key, value in d.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def EV(d: dict[float, float], normalize: bool = True) -> float:\n",
    "    \"\"\"Compute the expected value of a dict mapping `float`s to their probability or\n",
    "    to their weight (the latter if `normalize=True`)\"\"\"\n",
    "    if normalize:\n",
    "        d = make_probs_sum_to_one(d)\n",
    "    return sum(key * value for key, value in d.items())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "avg_sections_per_course = EV(sections_per_course_distribution)\n",
    "avg_instructors_per_course = EV(instructors_per_course_distribution)\n",
    "avg_sections_per_instructor = EV(sections_per_instructor_distribution)\n",
    "avg_students_per_section = EV(students_per_section_distribution)\n",
    "avg_courses_per_student = EV(num_courses_per_student_distribution)\n",
    "avg_credit_hours_per_student = EV(credit_hours_per_student_distribution)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_from_dict(\n",
    "    distribution: dict[Any, float],\n",
    "    size: int | None = None,\n",
    "    normalize: bool = True,\n",
    "    replace: bool = True,\n",
    ") -> Any:\n",
    "    \"\"\"Take a sample from a `dict`, where the keys are the sample space and the\n",
    "    values are the weights of each key,\"\"\"\n",
    "    if normalize:\n",
    "        distribution = make_probs_sum_to_one(distribution)\n",
    "    keys, weights = zip(*distribution.items())\n",
    "    return np.random.choice(keys, p=weights, size=size, replace=replace)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the number of unique students based on the average number of courses per\n",
    "# student\n",
    "n_unique_students = int(n_records / avg_courses_per_student)\n",
    "\n",
    "# Generate unique student data\n",
    "unique_students = pd.DataFrame(\n",
    "    {\n",
    "        \"Student ID\": [\n",
    "            # The builtin `random.sample` method from the Python standard lib can\n",
    "            # efficiently sample from `range` objects, while `np.random.sample` needs to\n",
    "            # construct the entire list in memory first.\n",
    "            f\"ID{rand_id:09}\"\n",
    "            for rand_id in random.sample(range(10**9), n_unique_students)\n",
    "        ],\n",
    "        \"Student Alternate ID\": np.NaN,\n",
    "        \"Student Name\": [\n",
    "            f\"{fake[locale].last_name()}, {fake[locale].first_name()}\"\n",
    "            for locale in sample_from_dict(locales_dict, size=n_unique_students)\n",
    "        ],\n",
    "        \"Classification\": sample_from_dict(\n",
    "            classifications_distribution, size=n_unique_students\n",
    "        ),\n",
    "        \"Major\": np.random.choice(\n",
    "            [\n",
    "                \"Computer Science\",\n",
    "                \"Business Administration\",\n",
    "                \"Psychology\",\n",
    "                \"Education\",\n",
    "                \"Engineering\",\n",
    "            ],\n",
    "            n_unique_students,\n",
    "        ),\n",
    "        \"Credit Hours\": sample_from_dict(\n",
    "            credit_hours_per_student_distribution, size=n_unique_students\n",
    "        ),\n",
    "    }\n",
    ").replace(\"None\", np.NaN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_emails(\n",
    "    names_series: pd.Series, existing_emails: pd.Series | None = None\n",
    ") -> pd.Series:\n",
    "    \"\"\"Generate emails based on a list of `student_names` without duplicating any\n",
    "    values in `existing_emails`\"\"\"\n",
    "\n",
    "    if existing_emails is None:\n",
    "        existing_emails = pd.Series()\n",
    "\n",
    "    # If `existing_emails` has duplicates, raise a `ValueError`\n",
    "    if existing_emails.duplicated().any():\n",
    "        duplicated_values = \", \".join(\n",
    "            existing_emails[existing_emails.duplicated()].unique()\n",
    "        )\n",
    "        raise ValueError(\n",
    "            f\"`existing_emails` contains duplicate addresses: {duplicated_values}\"\n",
    "        )\n",
    "\n",
    "    names = names_series.str.extract(r\"^(?P<last>[\\w '\\-]+), (?P<first>[\\w '\\-]+)$\")\n",
    "\n",
    "    # Convert all names to ASCII equivalents\n",
    "    # https://chat.openai.com/share/7db700d9-5647-44f4-927c-85b76efa1e9f\n",
    "    first_names_ascii = (\n",
    "        names[\"first\"]\n",
    "        .str.normalize(\"NFKD\")\n",
    "        .str.encode(\"ASCII\", \"ignore\")\n",
    "        .astype(str)\n",
    "        .str.replace(r\"[^A-Za-z0-9]\", \"\", regex=True)\n",
    "        .str.lower()\n",
    "    )\n",
    "    last_names_ascii = (\n",
    "        names[\"last\"]\n",
    "        .str.normalize(\"NFKD\")\n",
    "        .str.encode(\"ASCII\", \"ignore\")\n",
    "        .astype(str)\n",
    "        .str.replace(r\"[^A-Za-z0-9]\", \"\", regex=True)\n",
    "        .str.lower()\n",
    "    )\n",
    "\n",
    "    base_emails = first_names_ascii + \".\" + last_names_ascii\n",
    "    # Get the existing base-- that is, the part before \"@example.edu\"\n",
    "    existing_base_emails = existing_emails.str.extract(\n",
    "        r\"(.+)@example\\.edu\", expand=False\n",
    "    )\n",
    "\n",
    "    # Concatenate and reset index to align with base_emails\n",
    "    all_emails = pd.concat([existing_base_emails, base_emails], ignore_index=True)\n",
    "    # Remove periods for comparison\n",
    "    all_emails_normalized = all_emails.str.replace(\".\", \"\").str.lower()\n",
    "    # Find duplicates in normalized emails\n",
    "    duplicates = all_emails_normalized.duplicated(keep=\"first\")\n",
    "\n",
    "    number = 2  # Start numbering at 2\n",
    "    while duplicates.any():\n",
    "        # Extract the beginning part of the email base, up to (but ignoring) the number\n",
    "        all_emails.loc[duplicates] = all_emails[duplicates].str.extract(\n",
    "            r\"^([A-Za-z\\.]+)\\d?\", expand=False\n",
    "        ) + str(number)\n",
    "        all_emails_normalized = all_emails.str.replace(\".\", \"\").str.lower()\n",
    "        duplicates = all_emails_normalized.duplicated(keep=\"first\")\n",
    "        number += 1\n",
    "\n",
    "    # Be sure to only return the part of `all_emails` that's AFTER the `existing_emails`\n",
    "    new_emails = all_emails[len(existing_emails) :] + \"@example.edu\"\n",
    "    return new_emails.reset_index(drop=True)\n",
    "\n",
    "\n",
    "unique_students[\"Student E-mail\"] = generate_emails(unique_students[\"Student Name\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "existing_emails = unique_students[\"Student E-mail\"].copy()\n",
    "existing_ids = unique_students[\"Student ID\"].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate Categories\n",
    "def select_categories() -> str | None:\n",
    "    categories = []\n",
    "\n",
    "    def _include_category(chance: float = 0.9) -> bool:\n",
    "        \"\"\"Randomly decide to include a category or not\"\"\"\n",
    "        return np.random.rand() < chance\n",
    "\n",
    "    campus_options = {\"On-line\": 0.5, \"Main Campus\": 0.4, \"Satellite Campus\": 0.1}\n",
    "    graduated_options = {  # Mutually exclusive with certain other categories\n",
    "        \"Graduated: Yes\": 0.1,\n",
    "        \"Graduated: No\": 0.9,\n",
    "    }\n",
    "    level_options = [\n",
    "        \"Undergraduate\",\n",
    "        \"Graduate\",\n",
    "    ]\n",
    "    hold_types = [\"Hold: Financial\", \"Hold: Academic\", \"Hold: Administrative\"]\n",
    "    comp_rate_options = {\n",
    "        \"Completion Rate: >= 66.67%\": 0.75,\n",
    "        \"Completion Rate: < 66.67%\": 0.25,\n",
    "    }\n",
    "    start_term_options = [\n",
    "        f\"Start Term: {season} {year}\"\n",
    "        for year in range(2000, 2025)\n",
    "        for season in [\"Fall\", \"Winter\", \"Spring\", \"Summer\"]\n",
    "    ]\n",
    "    start_term_options_dict = {\n",
    "        term: float(i) for i, term in enumerate(start_term_options, start=1)\n",
    "    }\n",
    "    term_status_options = [  # Mutually exclusive with \"Graduated: Yes\"\n",
    "        \"Term Status: Registered\",\n",
    "        \"Term Status: Not Registered\",\n",
    "    ]\n",
    "    fafsa_options = [  # Mutually exclusive with \"Graduated: Yes\"\n",
    "        \"FAFSA: Yes\",\n",
    "        \"FAFSA: No\",\n",
    "    ]\n",
    "\n",
    "    # Build up categories one-by-one, randomly deciding which will be included.\n",
    "    if _include_category():\n",
    "        categories.append(f\"Campus: {sample_from_dict(campus_options)}\")\n",
    "    if _include_category():\n",
    "        categories.append(sample_from_dict(graduated_options))\n",
    "    if _include_category():\n",
    "        categories.append(np.random.choice(level_options))\n",
    "    if _include_category(chance=0.10):  # Only 10% chance of showing a Hold\n",
    "        categories.append(np.random.choice(hold_types))\n",
    "    if _include_category():\n",
    "        categories.append(sample_from_dict(comp_rate_options))\n",
    "    if _include_category():\n",
    "        categories.append(sample_from_dict(start_term_options_dict))\n",
    "\n",
    "    if \"Graduated: Yes\" not in categories:\n",
    "        # We can only be here if you do NOT show \"Graduated: Yes\"\n",
    "        if _include_category():\n",
    "            categories.append(np.random.choice(term_status_options))\n",
    "        if _include_category():\n",
    "            categories.append(np.random.choice(fafsa_options))\n",
    "\n",
    "    return \", \".join(categories) if categories else None\n",
    "\n",
    "\n",
    "unique_students[\"Categories\"] = [\n",
    "    select_categories() for _ in range(len(unique_students))\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_tags() -> str | None:\n",
    "    tags = []\n",
    "\n",
    "    # General tags\n",
    "    general_tags = [\n",
    "        \"Honor Student\",\n",
    "        \"Scholarship Recipient\",\n",
    "        \"At Risk\",\n",
    "        \"Needs Tutoring\",\n",
    "        \"Athlete\",\n",
    "        \"International\",\n",
    "        \"Transfer\",\n",
    "    ]\n",
    "    # Tuples of mutually exclusive tags\n",
    "    mutually_exclusive_tags = [(\"Part-Time\", \"Full-Time\")]\n",
    "\n",
    "    max_tags = len(general_tags) + len(mutually_exclusive_tags)\n",
    "\n",
    "    def _include_tag(chance: float = 1 - 0.675 ** (1 / max_tags)) -> bool:\n",
    "        \"\"\"Randomly decide to include a tag or not.\n",
    "        We want a 67.5% chance that a student has no tags whatsoever.\n",
    "        With 8 possible tags, a little algebra reveals that the default chance must be\n",
    "        `1 - 0.675 ** (1 / num_possible_tags)`.\"\"\"\n",
    "        return np.random.rand() < chance\n",
    "\n",
    "    for tag in general_tags:\n",
    "        if _include_tag():\n",
    "            tags.append(tag)\n",
    "\n",
    "    for tuple_of_mutually_exclusive_tags in mutually_exclusive_tags:\n",
    "        if _include_tag():\n",
    "            tags.append(np.random.choice(tuple_of_mutually_exclusive_tags))\n",
    "\n",
    "    return \", \".join(tags) if tags else None\n",
    "\n",
    "\n",
    "unique_students[\"Tags\"] = [select_tags() for _ in range(len(unique_students))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_cumulative_gpas(\n",
    "    length: int = 1,\n",
    ") -> np.typing.NDArray[np.floating[np.typing.NBitBase]]:\n",
    "    # Probabilities\n",
    "    prob_of_zero_gpa = 0.12\n",
    "    prob_of_high_gpa = 0.75 * (1 - prob_of_zero_gpa)  # 75% of non-zero\n",
    "\n",
    "    # Distribution counts\n",
    "    zero_gpas_count = np.random.binomial(length, prob_of_zero_gpa)\n",
    "    high_gpas_count = np.random.binomial(length - zero_gpas_count, prob_of_high_gpa)\n",
    "    low_gpas_count = length - zero_gpas_count - high_gpas_count\n",
    "\n",
    "    # Generate GPAs\n",
    "    zero_gpas = np.zeros(zero_gpas_count)\n",
    "    high_gpas = np.clip(np.random.normal(3.9, 0.4, size=high_gpas_count), 0, 4)\n",
    "    low_gpas = np.random.uniform(low=0, high=2.6, size=low_gpas_count)\n",
    "\n",
    "    # Combine and shuffle\n",
    "    all_gpas = np.concatenate([zero_gpas, high_gpas, low_gpas]).round(2)\n",
    "    np.random.shuffle(all_gpas)\n",
    "\n",
    "    all_gpas\n",
    "\n",
    "    return np.round(all_gpas, 2)\n",
    "\n",
    "\n",
    "unique_students[\"Cumulative GPA\"] = generate_cumulative_gpas(len(unique_students))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add \"Assigned Staff\"\n",
    "staff_df = pd.DataFrame()\n",
    "staff_df[\"name\"] = [\n",
    "    f\"{fake[locale].last_name()}, {fake[locale].first_name()}\"\n",
    "    for locale in sample_from_dict(locales_dict, n_unique_students // 40)\n",
    "]\n",
    "\n",
    "staff_df[\"email\"] = generate_emails(staff_df[\"name\"], existing_emails=existing_emails)\n",
    "# Update the `existing_emails`\n",
    "existing_emails = pd.concat([existing_emails, staff_df[\"email\"]])\n",
    "\n",
    "staff_df[\"id\"] = pd.NA\n",
    "while staff_df[\"id\"].isna().any():\n",
    "    # Repeat the ID number assignment if anyone overlaps with a Student ID (even tho\n",
    "    # this is EXTREMELY unlikely!)\n",
    "    staff_df[\"id\"] = [\n",
    "        (f\"ID{rand_id:09}\" if f\"ID{rand_id:09}\" not in existing_ids else pd.NA)\n",
    "        for rand_id in random.sample(range(10**9), len(staff_df))\n",
    "    ]\n",
    "# Update the `existing_ids`\n",
    "existing_ids = pd.concat([existing_ids, staff_df[\"id\"]])\n",
    "\n",
    "\n",
    "assigned_staff_role_probabilities_no_professor = {\n",
    "    role: prob\n",
    "    for role, prob in assigned_staff_role_probabilities.items()\n",
    "    if role.casefold() != \"professor\"\n",
    "}\n",
    "staff_df[\"role\"] = sample_from_dict(\n",
    "    assigned_staff_role_probabilities_no_professor, len(staff_df)\n",
    ")\n",
    "\n",
    "\n",
    "def select_assigned_staff(length: int = 1) -> np.typing.NDArray[np.str_]:\n",
    "    assigned_staff = pd.Series(dtype=\"string\", index=range(length))\n",
    "    for staff_role, prob in assigned_staff_role_probabilities_no_professor.items():\n",
    "        this_role_df = staff_df[staff_df[\"role\"] == staff_role]\n",
    "        mask_has_this_staff = np.random.rand(length) < prob\n",
    "        n_has_this_staff = mask_has_this_staff.sum()\n",
    "\n",
    "        chosen_staff = pd.Series(\n",
    "            np.random.choice(this_role_df[\"name\"], n_has_this_staff)\n",
    "        )\n",
    "\n",
    "        staff_name_formatted = chosen_staff.str.replace(\n",
    "            r\"^(?P<last>[\\w '\\-]+), (?P<first>[\\w '\\-]+)$\",\n",
    "            rf\"\\g<first> \\g<last> ({staff_role})\",\n",
    "            regex=True,\n",
    "        )\n",
    "\n",
    "        mask_this_is_my_first_staff = assigned_staff.isna() & mask_has_this_staff\n",
    "        n_this_is_my_first_staff = mask_this_is_my_first_staff.sum()\n",
    "        assigned_staff.loc[mask_this_is_my_first_staff & mask_has_this_staff] = (\n",
    "            staff_name_formatted[:n_this_is_my_first_staff].values\n",
    "        )\n",
    "        assigned_staff.loc[~mask_this_is_my_first_staff & mask_has_this_staff] += (\n",
    "            \", \" + staff_name_formatted[n_this_is_my_first_staff:].values\n",
    "        )\n",
    "\n",
    "    return assigned_staff.values\n",
    "\n",
    "\n",
    "unique_students[\"Assigned Staff\"] = select_assigned_staff(n_unique_students)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_courses_per_student = sample_from_dict(\n",
    "    num_courses_per_student_distribution, size=n_unique_students\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replicate each student entry based on the number of courses they're taking\n",
    "replicated_students = unique_students.loc[\n",
    "    unique_students.index.repeat(num_courses_per_student)\n",
    "].reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Course info\n",
    "# Generate a course schedule to assign students courses\n",
    "n_course_numbers_needed = np.ceil(\n",
    "    n_records / avg_students_per_section / avg_sections_per_course\n",
    ").astype(int)\n",
    "\n",
    "n_instructors_needed = np.ceil(\n",
    "    n_course_numbers_needed / avg_sections_per_instructor\n",
    ").astype(int)\n",
    "\n",
    "n_sections_per_course = sample_from_dict(\n",
    "    sections_per_course_distribution, size=n_course_numbers_needed\n",
    ")\n",
    "\n",
    "section_id_nums = np.random.choice(\n",
    "    np.arange(10**4, 10**5), size=sum(n_sections_per_course), replace=False\n",
    ")\n",
    "\n",
    "course_depts = (\n",
    "    \"AAS,AHS,ANT,ART,CAN,CIN,COM,CSC,DES,EAS,EDL,EDU,ENG,FME,GEN,HIS,HSC,LAN,LAS,LAW,\"\n",
    "    \"MED,MTH,MUS,NUR,OBG,OPH,PHA,PHE,PHY,PTH,QRM,RAC,ROM,SCI,SCW,SLV,SOC,STT,SUR,THR,\"\n",
    "    \"URB,VIA\"\n",
    ").split(\",\")\n",
    "course_number_ints = sample_from_dict(\n",
    "    {\n",
    "        i: 1 / i  # Higher course numbers map to lower probabilities\n",
    "        for i in range(95, 601)  # Course numbers can fall between 95 and 601\n",
    "    },\n",
    "    size=180,\n",
    "    replace=False,\n",
    ")\n",
    "\n",
    "\n",
    "# Add some course numbers that we definitely want to see\n",
    "course_numbers_set = set(course_number_ints) | {101, 200, 600, \"115L\", \"105H\"}\n",
    "course_numbers_list = sorted(f\"{x:03}\" for x in course_numbers_set)\n",
    "\n",
    "\n",
    "course_numbers = (\n",
    "    pd.Series(\n",
    "        f\"{course_dept}-{course_number:03}\"\n",
    "        for course_dept in course_depts\n",
    "        for course_number in course_numbers_list\n",
    "    )\n",
    "    .sample(n_course_numbers_needed)\n",
    "    .sort_values(ignore_index=True)\n",
    ")\n",
    "\n",
    "course_names = pd.Series(\n",
    "    fake[\"en_US\"].unique.sentence(nb_words=6) for _ in range(n_course_numbers_needed)\n",
    ").str.replace(\n",
    "    r\"\\.$\", \"\", regex=True  # strip the final period\n",
    ")\n",
    "\n",
    "\n",
    "course_schedule_list = []\n",
    "for course_number, course_name, num_course_sections, section_id_num in zip(\n",
    "    course_numbers, course_names, n_sections_per_course, section_id_nums\n",
    "):\n",
    "    for _section_num in range(num_course_sections):\n",
    "        course_schedule_list.append(\n",
    "            {\n",
    "                \"Course Number\": course_number,\n",
    "                \"Course Name\": course_name,\n",
    "                \"Section\": section_id_num,\n",
    "            }\n",
    "        )\n",
    "\n",
    "course_schedule = pd.DataFrame(course_schedule_list)\n",
    "\n",
    "course_schedule"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "instructors_df = pd.DataFrame()\n",
    "instructors_df[\"instructor_names\"] = pd.Series(\n",
    "    [\n",
    "        f\"{fake[locale].last_name()}, {fake[locale].first_name()}\"\n",
    "        for locale in sample_from_dict(locales_dict, size=n_instructors_needed)\n",
    "    ]\n",
    ")\n",
    "instructors_df[\"instructor_emails\"] = generate_emails(\n",
    "    instructors_df[\"instructor_names\"],\n",
    "    existing_emails=existing_emails,\n",
    ")\n",
    "existing_emails = pd.concat([existing_emails, instructors_df[\"instructor_emails\"]])\n",
    "\n",
    "instructors_df[\"instructor_id\"] = pd.NA\n",
    "while instructors_df[\"instructor_id\"].isna().any():\n",
    "    # Repeat the ID number assignment if anyone overlaps with a Student ID or Staff ID\n",
    "    # (even tho this is EXTREMELY unlikely!)\n",
    "    instructors_df[\"instructor_id\"] = [\n",
    "        f\"ID{rand_id:09}\" if f\"ID{rand_id:09}\" not in existing_ids else pd.NA\n",
    "        for rand_id in random.sample(range(10**9), n_instructors_needed)\n",
    "    ]\n",
    "existing_ids = pd.concat([existing_ids, instructors_df[\"instructor_id\"]])\n",
    "\n",
    "\n",
    "instructors_df[\"is_assignable_as_staff\"] = np.random.choice(\n",
    "    [True, False],\n",
    "    size=len(instructors_df),\n",
    "    p=[\n",
    "        assigned_staff_role_probabilities[\"Professor\"],\n",
    "        1 - assigned_staff_role_probabilities.pop(\"Professor\"),\n",
    "    ],\n",
    ")\n",
    "\n",
    "instructors_df[\"name_email_formatted\"] = (\n",
    "    instructors_df[\"instructor_names\"]\n",
    "    + \" (\"\n",
    "    + instructors_df[\"instructor_id\"]\n",
    "    + \") <\"\n",
    "    + instructors_df[\"instructor_emails\"]\n",
    "    + \">\"\n",
    ")\n",
    "\n",
    "num_instructors_per_section = sample_from_dict(\n",
    "    instructors_per_course_distribution, len(course_schedule)\n",
    ")\n",
    "\n",
    "course_schedule[\"Instructors\"] = [\n",
    "    (\n",
    "        \"; \".join(instructors_df[\"name_email_formatted\"].sample(n).sort_values())\n",
    "        if n > 0\n",
    "        else None\n",
    "    )\n",
    "    for n in num_instructors_per_section\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Schedule information\n",
    "# Start Date & End Date\n",
    "course_schedule[\"Start Date\"] = [\n",
    "    fake.date_between(start_date=\"-3m\", end_date=\"+3m\")\n",
    "    for _ in range(len(course_schedule))\n",
    "]\n",
    "course_schedule[\"End Date\"] = course_schedule[\"Start Date\"] + pd.Timedelta(weeks=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start Time & End Time\n",
    "course_schedule[\"Start Time\"] = sample_from_dict(\n",
    "    start_times_distribution, size=len(course_schedule)\n",
    ")\n",
    "\n",
    "\n",
    "LATEST_POSSIBLE_END_TIME = pd.to_datetime(\"10:00 PM\", format=\"%I:%M %p\")\n",
    "\n",
    "potential_end_times_with_arbitrary_date = (\n",
    "    pd.to_datetime(\n",
    "        course_schedule[\"Start Time\"].str.replace(\" CT\", \"\"), format=\"%I:%M %p\"\n",
    "    )\n",
    "    + (\n",
    "        sample_from_dict(\n",
    "            course_duration_distribution, size=len(course_schedule)\n",
    "        ).astype(\"timedelta64\")\n",
    "    )\n",
    ").rename(\"Potential End Times\")\n",
    "\n",
    "potential_end_times_with_arbitrary_date.loc[\n",
    "    potential_end_times_with_arbitrary_date > LATEST_POSSIBLE_END_TIME\n",
    "] = LATEST_POSSIBLE_END_TIME\n",
    "\n",
    "course_schedule[\"End Time\"] = (\n",
    "    potential_end_times_with_arbitrary_date.dt.strftime(\"%I:%M %p\")\n",
    "    .str.lstrip(\"0\")\n",
    "    .rename(\"End Time\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Class Days\n",
    "course_schedule[\"Class Days\"] = sample_from_dict(\n",
    "    class_days_distribution, len(course_schedule)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "student_course_assignments_list = []\n",
    "num_courses_for_each_student = sample_from_dict(\n",
    "    num_courses_per_student_distribution, size=n_unique_students\n",
    ")\n",
    "\n",
    "for student_id, num_courses in zip(\n",
    "    unique_students[\"Student ID\"], num_courses_for_each_student\n",
    "):\n",
    "    selected_sections = course_schedule.sample(n=num_courses)\n",
    "\n",
    "    # for index\n",
    "    # courses = course_schedule[course_schedule[\"Section\"].isin(selected_sections)]\n",
    "    for _index, selection_row in selected_sections.iterrows():\n",
    "        student_course_assignments_list.append(\n",
    "            {\"Student ID\": student_id} | {**selection_row}\n",
    "        )\n",
    "\n",
    "\n",
    "replicated_students = pd.DataFrame(student_course_assignments_list).merge(\n",
    "    right=unique_students, how=\"left\", validate=\"many_to_one\"\n",
    ")\n",
    "\n",
    "replicated_students"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add Professors to \"Assigned Staff\"\n",
    "# Explode the instructors list, since some sections will have multiple instructors\n",
    "exploded_instructors = (\n",
    "    replicated_students[\"Instructors\"].str.split(\"; \").explode().dropna().to_frame()\n",
    ")\n",
    "\n",
    "# Add the student IDs to each row, aligning on the index\n",
    "exploded_instructors[\"Student ID\"] = replicated_students[\"Student ID\"]\n",
    "\n",
    "# Merge the info from the `instructors_df`\n",
    "# we only care about those instructors who are assignable as staff\n",
    "merged_enrollment_instructor_data = exploded_instructors.merge(\n",
    "    instructors_df[instructors_df[\"is_assignable_as_staff\"]],\n",
    "    left_on=\"Instructors\",\n",
    "    right_on=\"name_email_formatted\",\n",
    "    how=\"inner\",\n",
    ")\n",
    "\n",
    "# Use a regex to format the names how we'd like for the \"Assigned Staff\" column\n",
    "merged_enrollment_instructor_data[\n",
    "    \"Assigned Professors\"\n",
    "] = merged_enrollment_instructor_data[\"instructor_names\"].str.replace(\n",
    "    r\"^(?P<last>[\\w '\\-]+), (?P<first>[\\w '\\-]+)$\",\n",
    "    r\"\\g<first> \\g<last> (Professor)\",\n",
    "    regex=True,\n",
    ")\n",
    "\n",
    "# Group the staff by instructors and `\", \".join` them\n",
    "assigned_professors = merged_enrollment_instructor_data.groupby(\"Student ID\")[\n",
    "    \"Assigned Professors\"\n",
    "].agg(\", \".join)\n",
    "\n",
    "# Merge the assigned staff back to our enrollments report\n",
    "replicated_students[\"Assigned Staff\"] = (\n",
    "    replicated_students[[\"Student ID\", \"Assigned Staff\"]]\n",
    "    .merge(\n",
    "        assigned_professors,\n",
    "        how=\"left\",\n",
    "        left_on=\"Student ID\",\n",
    "        right_index=True,\n",
    "    )\n",
    "    .fillna(\"\")\n",
    "    .apply(\n",
    "        lambda x: x[\"Assigned Staff\"]\n",
    "        + (\", \" if x[\"Assigned Staff\"] and x[\"Assigned Professors\"] else \"\")\n",
    "        + x[\"Assigned Professors\"],\n",
    "        axis=\"columns\",\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Alphabetize each student's \"Assigned Staff\"\n",
    "replicated_students[\"Assigned Staff\"] = (\n",
    "    replicated_students[\"Assigned Staff\"].str.split(\", \").apply(sorted).apply(\", \".join)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enrollment info\n",
    "replicated_students[\"Dropped?\"] = sample_from_dict(\n",
    "    {\"Yes\": 0.24, \"No\": 0.76}, size=len(replicated_students)\n",
    ")\n",
    "replicated_students[\"Dropped Date\"] = dropped_dates = [\n",
    "    fake.date_between(start_date=\"-1y\", end_date=\"today\") if drop == \"Yes\" else None\n",
    "    for drop in replicated_students[\"Dropped?\"]\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add other enrollment-specific information (e.g., grades, attendance) in a similar\n",
    "# manner\n",
    "replicated_students[\"Midterm Grade\"] = np.random.choice(\n",
    "    [\"A\", \"B\", \"C\", \"D\", \"F\"], len(replicated_students)\n",
    ")\n",
    "replicated_students[\"Final Grade\"] = np.random.choice(\n",
    "    [\"A\", \"B\", \"C\", \"D\", \"F\"], len(replicated_students)\n",
    ")\n",
    "replicated_students[\"Total Progress Reports\"] = np.random.poisson(\n",
    "    0.4, len(replicated_students)\n",
    ")\n",
    "\n",
    "# The proportion of students with 0 absences\n",
    "PERFECT_ATTENDANCE_RATE = 0.85\n",
    "# Model the distribution of non-zero absences\n",
    "replicated_students[\"Absences\"] = replicated_students[\"Unexcused Absences\"] = (\n",
    "    np.random.poisson(2.84, size=len(replicated_students)).clip(min=1)\n",
    ")\n",
    "# Replace `PERFECT_ATTENDANCE_RATE` proportion of records with 0 absences\n",
    "replicated_students.loc[\n",
    "    np.random.rand(len(replicated_students)) > PERFECT_ATTENDANCE_RATE, \"Absences\"\n",
    "] = 0\n",
    "replicated_students[\"Excused Absences\"] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create DataFrame\n",
    "# and ensure that the dataframe is not longer than `n_records`\n",
    "df_unordered_columns = replicated_students.iloc[:n_records]\n",
    "\n",
    "# Order the columns as expected:\n",
    "df = df_unordered_columns[\n",
    "    [\n",
    "        \"Student Name\",\n",
    "        \"Student E-mail\",\n",
    "        \"Student ID\",\n",
    "        \"Student Alternate ID\",\n",
    "        \"Categories\",\n",
    "        \"Tags\",\n",
    "        \"Classification\",\n",
    "        \"Major\",\n",
    "        \"Cumulative GPA\",\n",
    "        \"Assigned Staff\",\n",
    "        \"Course Name\",\n",
    "        \"Course Number\",\n",
    "        \"Section\",\n",
    "        \"Instructors\",\n",
    "        \"Dropped?\",\n",
    "        \"Dropped Date\",\n",
    "        \"Midterm Grade\",\n",
    "        \"Final Grade\",\n",
    "        \"Total Progress Reports\",\n",
    "        \"Absences\",\n",
    "        \"Unexcused Absences\",\n",
    "        \"Excused Absences\",\n",
    "        \"Credit Hours\",\n",
    "        \"Start Date\",\n",
    "        \"End Date\",\n",
    "        \"Start Time\",\n",
    "        \"End Time\",\n",
    "        \"Class Days\",\n",
    "    ]\n",
    "].replace({None: np.NaN})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "EAB-tools-dev",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
